{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network with an LSTM unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Input, TimeDistributed, Dropout, Masking\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "B = 50  #  batch size\n",
    "R = 300 #  rnn size\n",
    "S = 4   #  max_sequence len\n",
    "E = 300 #  embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading and preparing data for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve fastText embeddings\n",
    "# vec_model = gensim.models.fasttext.FastText.load_fasttext_format('fasttext/cc.fi.300.bin')\n",
    "vec_model = gensim.models.KeyedVectors.load_word2vec_format('fasttext/crawl-300d-2M.vec', limit=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and validation examples for the network\n",
    "def generate(path, line_limit=13000, mode='train'):\n",
    "    with open(path, 'r', encoding='utf8') as src:\n",
    "        word_count = 0\n",
    "        line_number = 0\n",
    "        \n",
    "        x = np.zeros((B, S, E))\n",
    "        y = np.zeros((B, S, E))\n",
    "        word_seqs = [None for _ in range(B)]\n",
    "        lemma_seqs = [None for _ in range(B)]\n",
    "        word_seq = []\n",
    "        lemma_seq = []\n",
    "        x_seq = []\n",
    "        y_seq = []\n",
    "        i = 0\n",
    "        for line in src:\n",
    "            line_number += 1\n",
    "            if line_number > line_limit:\n",
    "                return        \n",
    "            if len(x_seq) == S and len(y_seq) == S:\n",
    "                x[i] = np.array(x_seq)\n",
    "                y[i] = np.array(y_seq)\n",
    "                word_seqs[i] = word_seq[:]\n",
    "                lemma_seqs[i] = lemma_seq[:] \n",
    "                if mode == 'train':\n",
    "                    x_seq.pop(0)\n",
    "                    y_seq.pop(0)\n",
    "                    word_seq.pop(0)\n",
    "                    lemma_seq.pop(0)\n",
    "                else:\n",
    "                    x_seq = []\n",
    "                    y_seq = []\n",
    "                    word_seq = []\n",
    "                    lemma_seq = []\n",
    "                i += 1\n",
    "                if i >= B:\n",
    "                    yield x, y, word_seqs, lemma_seqs\n",
    "                    x = np.zeros((B, S, E))\n",
    "                    y = np.zeros((B, S, E))\n",
    "                    word_seqs = [None for _ in range(B)]\n",
    "                    lemma_seqs = [None for _ in range(B)]\n",
    "                    i = 0\n",
    "                    word_count += S\n",
    "            if len(line) > 2 and line[0] != '#':\n",
    "                values = line.split()\n",
    "                if '-' not in values[0]:\n",
    "                    try:\n",
    "                        word = vec_model[values[1]]\n",
    "                        lemma_vec = vec_model[values[2]]\n",
    "                    except:\n",
    "                        word = np.zeros(E)\n",
    "                        lemma_vec = np.zeros(E)\n",
    "                    x_seq.append(word)\n",
    "                    y_seq.append(lemma_vec)\n",
    "                    word_seq.append(values[1])\n",
    "                    lemma_seq.append(values[2])                        \n",
    "            else:\n",
    "                x_seq = []\n",
    "                y_seq = []\n",
    "                word_seq = []\n",
    "                lemma_seq = []                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.2k lines amount to 10k tokens\n",
    "train_set = [(X, Y) for X, Y, _, _ in generate('UD_English-EWT/en_ewt-ud-train.conllu', line_limit=12200)]\n",
    "\n",
    "# 2.5k lines amount to 2k tokens\n",
    "dev_batches = [(x, y, w, l) for x, y, w, l in generate('UD_English-EWT/en_ewt-ud-dev.conllu', line_limit=2500, mode='dev')]\n",
    "test_batches = [(x, y, w, l) for x, y, w, l in generate('UD_English-EWT/en_ewt-ud-test.conllu', line_limit=2500, mode='dev')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining and training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Sequential()\n",
    "M.add(Masking(mask_value=.0, input_shape=(S, E)))\n",
    "M.add(LSTM(R, return_sequences=True))\n",
    "M.add(Dropout(.2))\n",
    "M.add(TimeDistributed(Dense(E, activation='linear')))\n",
    "M.compile(loss='cosine_proximity', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \ttrain loss: -0.7171 \tdev loss: -0.8671\n",
      "epoch: 10 \ttrain loss: -0.9192 \tdev loss: -0.9474\n",
      "epoch: 20 \ttrain loss: -0.9262 \tdev loss: -0.9533\n",
      "epoch: 30 \ttrain loss: -0.9305 \tdev loss: -0.9555\n",
      "epoch: 40 \ttrain loss: -0.9333 \tdev loss: -0.9572\n",
      "epoch: 50 \ttrain loss: -0.9354 \tdev loss: -0.9583\n",
      "epoch: 60 \ttrain loss: -0.9369 \tdev loss: -0.9591\n",
      "epoch: 70 \ttrain loss: -0.9380 \tdev loss: -0.9592\n",
      "epoch: 80 \ttrain loss: -0.9390 \tdev loss: -0.9591\n",
      "epoch: 90 \ttrain loss: -0.9397 \tdev loss: -0.9594\n",
      "epoch: 100 \ttrain loss: -0.9400 \tdev loss: -0.9593\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_batch_c = 0\n",
    "    for X, Y in train_set:\n",
    "        train_loss += M.train_on_batch(X, Y)\n",
    "        train_batch_c += 1\n",
    "    \n",
    "    dev_loss = 0\n",
    "    dev_batch_c = 0\n",
    "    for X, Y, _, _ in dev_batches:\n",
    "        dev_loss += M.test_on_batch(X, Y)\n",
    "        dev_batch_c += 1\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('epoch:', epoch + 1, \n",
    "              '\\ttrain loss: {0:.4f}'.format(train_loss / train_batch_c), \n",
    "              '\\tdev loss: {0:.4f}'.format(dev_loss / dev_batch_c))\n",
    "\n",
    "    np.random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "count = 0\n",
    "for X, Y, W, L in test_batches:\n",
    "    pred = M.predict_on_batch(X)\n",
    "    for i, seq in enumerate(pred):\n",
    "        for j, pred_y in enumerate(seq):\n",
    "            if np.sum(X[i][j]) == 0:\n",
    "                nearest = W[i][j]  # identity backoff for oov tokens\n",
    "            else:\n",
    "                nearest = vec_model.most_similar(positive=[pred_y], topn=1)[0][0]\n",
    "            if nearest == L[i][j]:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "#             print('w', W[i][j], '\\tl', L[i][j], '\\tpred', nearest, nearest == L[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test accuracy: 93.00%\n",
      "correctly lemmatized tokens: 1860\n",
      "all tokens: 2000\n"
     ]
    }
   ],
   "source": [
    "print('final test accuracy: {0:.2f}%'.format(100 * correct / count))\n",
    "print('correctly lemmatized tokens:', correct)\n",
    "print('all tokens:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    \"\"\"\n",
    "    input: list of tokens\n",
    "    output: list of input tokens' predicted lemmas\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for i in range(0, len(tokens), S):\n",
    "        x = np.zeros((1, S, E))\n",
    "        oov = []\n",
    "        for j, t in enumerate(tokens[i:min(i + S, len(tokens))]):\n",
    "            try:\n",
    "                x[0][j] = vec_model[t]\n",
    "            except:\n",
    "                oov.append(j)\n",
    "        y = M.predict([x], batch_size=1)\n",
    "        predicted_lemmas = []\n",
    "        for j in range(min(i + S, len(tokens)) - i):\n",
    "            if j in oov:\n",
    "                predicted_lemmas.append(tokens[i + j])\n",
    "            else:\n",
    "                predicted_lemmas.append(vec_model.most_similar(positive=[y[0][j]], topn=1)[0][0])\n",
    "        lemmas += predicted_lemmas    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'know', 'he', 'because', 'he', 'have', 'attend', 'my', 'school', '.']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"I knew him because he had attended my school .\".split(' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
